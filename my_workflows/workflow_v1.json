{
  "edges": [
    {
      "source": "start",
      "target": "init_global"
    },
    {
      "source": "init_global",
      "target": "setup_workspace"
    },
    {
      "source": "setup_workspace",
      "target": "subloop_node"
    },
    {
      "source": "subloop_node",
      "target": "check_imp"
    },
    {
      "label": "true",
      "source": "check_imp",
      "target": "next_layer"
    },
    {
      "source": "lesson_node",
      "target": "next_attempt"
    },
    {
      "source": "next_layer",
      "target": "check_cycles"
    },
    {
      "source": "next_attempt",
      "target": "check_cycles"
    },
    {
      "label": "true",
      "source": "check_cycles",
      "target": "setup_workspace"
    },
    {
      "label": "false",
      "source": "check_cycles",
      "target": "end"
    },
    {
      "label": "false",
      "source": "check_imp",
      "target": "increment_failures"
    },
    {
      "source": "increment_failures",
      "target": "check_fail_threshold"
    },
    {
      "label": "false",
      "source": "check_fail_threshold",
      "target": "lesson_node"
    },
    {
      "label": "true",
      "source": "check_fail_threshold",
      "target": "restore_last_success"
    },
    {
      "source": "restore_last_success",
      "target": "meta_lesson"
    },
    {
      "source": "meta_lesson",
      "target": "meta_analysis_llm"
    },
    {
      "source": "reset_fail_count",
      "target": "next_attempt"
    },
    {
      "source": "meta_analysis_llm",
      "target": "n_1766633725413",
      "label": null
    },
    {
      "source": "n_1766633725413",
      "target": "n_1766633914418",
      "label": null
    },
    {
      "source": "n_1766633914418",
      "target": "reset_fail_count",
      "label": null
    }
  ],
  "global_vars": {
    "DEFAULT_SYS": "  You are an expert AI Quantitative Researcher specializing in Deep Learning for Market Regime\n  Modeling.\n\n  Your Goal:\n  Minimize the NLL Loss Score (Target -> 0.0) of a CNN+LSTM model that predicts the\n  Time-to-Next-Event (Peak or Trough) in BTC/USDT price action.\n   * Metric: NLL Loss Score. Lower is better.\n\n  Workflow:\n   1. Modify `extract_features` in `strategy.py`: Combine technical indicators (TALib),\n      price/volume transforms, and temporal features.\n   2. Evaluate: The external system runs evaluator.py, which trains your model on historical data\n      and tests it via Walk-Forward Analysis.\n   3. Iterate: Use the feedback (NLL Loss Score) to refine your feature set. If the score is\n      high, your features are not informative enough.\n\n  Constraints:\n   * Stationarity: Inputs must be relative/normalized (e.g., Price / MA, not raw Price).\n   * Causality: No future data peeking.\n   * Stability: Avoid NaN/Inf values. Use log1p for skewed distributions like Volume or Time.",
    "venv": "/home/liumx/.conda/envs/lean/bin/python",
    "plot_names": "@btcusdt_WFA_1.png, @btcusdt_WFA_6.png, @btcusdt_WFA_7.png"
  },
  "id": "root_graph",
  "llm_changeable_vars": [
    "hypothesis",
    "exp_design",
    "result_analysis",
    "hint"
  ],
  "nodes": [
    {
      "config": {},
      "id": "start",
      "label": "Start",
      "position": {
        "x": 50,
        "y": 300
      },
      "type": "start"
    },
    {
      "config": {
        "code": "# Initialize Global Counters\ncontext['cycle'] = 0\ncontext['branch_idx'] = context.get('branch_idx', 1)\n\nbranch_path = service.tasks_dir / f\"Branch{context['branch_idx']}\"\nvalid, last_imp, last_att = service.scan_experiments(branch_path)\n\n# Clear previous metrics\ncontext['parent_metric'] = None\ncontext['lessons_text'] = ''\ncontext['consecutive_failures'] = 0\n\nparent_path_to_read = None\n\nif last_att:\n    # Resume Mode: Calculate next step based on history\n    nl, ns, pp = service.generate_next_node(context['branch_idx'], last_imp, last_att)\n    context['layer_idx'] = nl\n    context['sub_idx'] = ns\n    \n    if pp:\n        parent_path_to_read = pp\n        context['next_parent_path'] = str(pp)\n    elif last_imp:\n        # Fallback to last improvement if valid\n        parent_path_to_read = last_imp['path']\n        # Ensure we don't carry over a stale next_parent_path if pp was None\n        if 'next_parent_path' in context: del context['next_parent_path']\n\n    logger.log(f\"\ud83d\udd04 Resuming Branch {context['branch_idx']} at {nl}.{ns}\")\nelse:\n    # New Branch Mode\n    context['layer_idx'] = 1\n    context['sub_idx'] = 1\n    \n    # Check if a parent path is specified in context\n    if context.get('next_parent_path'):\n        parent_path_to_read = Path(context['next_parent_path'])\n        logger.log(f\"\ud83d\udd0d Init Global: Using specified parent path: {parent_path_to_read}\")\n    else:\n        # Fallback to default example\n        example_path = service.tasks_dir / 'Branch_example' / 'exp_example'\n        if example_path.exists():\n            parent_path_to_read = example_path\n            context['next_parent_path'] = str(example_path)\n            logger.log(f\"\ud83d\udd0d Init Global: Using default example parent: {example_path}\")\n    \n    logger.log(f\"\u2728 Starting New Branch {context['branch_idx']} at 1.1\")\n\n# Attempt to read metric from determined parent path\nif parent_path_to_read:\n    p_path = Path(parent_path_to_read)\n    history_file = p_path / 'history.json'\n    logger.log(f\"\ud83d\udd0d Init Global: Attempting to read metric from: {history_file}\")\n    \n    if history_file.exists():\n        try:\n            with open(history_file, 'r') as f:\n                ph = json.load(f)\n                pm = ph.get('metrics')\n                \n                # Handle dict format\n                if isinstance(pm, dict): \n                    pm = pm.get('metric_score')\n                \n                # Convert to float\n                try:\n                    if pm is not None:\n                        pm = float(pm)\n                except (ValueError, TypeError):\n                    logger.log(f\"\u26a0\ufe0f Init Global: Could not cast metric '{pm}' to float. Setting to None.\")\n                    pm = None\n                    \n                context['parent_metric'] = pm\n                if pm is not None:\n                    logger.log(f\"\ud83d\udcca Init Global: Successfully loaded Parent Metric {pm}\")\n        except Exception as e:\n            logger.log(f\"\u26a0\ufe0f Init Global: Error reading {history_file}: {e}\")\n            context['parent_metric'] = None\n    else:\n        logger.log(f\"\u2139\ufe0f Init Global: History file not found at {history_file}\")\n        context['parent_metric'] = None\nelse:\n    logger.log(\"\u2139\ufe0f Init Global: No parent path available to read metric.\")\n\nlogger.log('\ud83c\udf0d Global Context Initialized')"
      },
      "id": "init_global",
      "label": "Init Global (L=1, S=1)",
      "position": {
        "x": 200,
        "y": 300
      },
      "type": "python_script"
    },
    {
      "config": {
        "code": "# Setup Workspace for Current L.S\nb_idx = context['branch_idx']\nl = context['layer_idx']\ns = context['sub_idx']\ncontext['cycle'] += 1\n\nlogger.log(f'\ud83d\ude80 === CYCLE {context[\"cycle\"]} (Exp {b_idx}.{l}.{s}) ===')\n\nbranch_path = service.tasks_dir / f'Branch{b_idx}'\n\n# Determine Parent Path\nif 'next_parent_path' in context:\n    parent_path = Path(context['next_parent_path'])\nelse:\n    # First run fallback\n    valid, last_imp, last_att = service.scan_experiments(branch_path)\n    parent_path = None \n    if last_imp: parent_path = last_imp['path']\n\n# Read Parent Metric AND Hint\ncontext['parent_metric'] = None\ncontext['hint'] = ''\n\nif parent_path and (parent_path / 'history.json').exists():\n    try:\n        with open(parent_path / 'history.json', 'r') as f:\n            ph = json.load(f)\n            \n            # 1. Load Metric\n            pm = ph.get('metrics')\n            if isinstance(pm, dict): pm = pm.get('metric_score') # Check for 'metric_score'\n            context['parent_metric'] = pm\n            logger.log(f'\\ud83d\\udcca Loaded Parent Metric: {pm}')\n            \n            # 2. Load HINT\n            hint_val = ph.get('hint', '')\n            if hint_val:\n                 context['hint'] = hint_val\n                 logger.log(f'\\ud83d\\udca1 Loaded Parent hint: {hint_val[:50]}...')\n    except: pass\n\ncur_path = service.setup_workspace(branch_path, l, s, parent_path, b_idx)\n\n# FIX: Reload hint for first node (if empty)\nif not context.get('hint') and (cur_path / 'history.json').exists():\n    try:\n        with open(cur_path / 'history.json', 'r') as f:\n            h = json.load(f)\n            if h.get('hint'):\n                context['hint'] = h['hint']\n                logger.log(f\"\\ud83d\\udca1 Loaded Initial Hint from Current: {context['hint'][:50]}...\")\n    except: pass\ncontext['current_exp_path'] = str(cur_path)\nlogger.log(f'\ud83d\udcc2 Workspace Prepared: {cur_path}')"
      },
      "flip": true,
      "id": "setup_workspace",
      "label": "Setup Workspace",
      "position": {
        "x": 593,
        "y": 74
      },
      "type": "python_script"
    },
    {
      "config": {
        "sub_graph": {
          "edges": [
            {
              "source": "sub_start",
              "target": "step1_impl"
            },
            {
              "source": "step1_impl",
              "target": "step1_save_session"
            },
            {
              "source": "step4_eval",
              "target": "step4_5_parse"
            },
            {
              "source": "step4_5_parse",
              "target": "step5_check"
            },
            {
              "source": "step5_check",
              "target": "step5_1_check_retry"
            },
            {
              "source": "step6_save_metric",
              "target": "step7_save_imp"
            },
            {
              "source": "step7_save_imp",
              "target": "step8_check_threshold"
            },
            {
              "label": "true",
              "source": "step8_check_threshold",
              "target": "step9_supervisor"
            },
            {
              "source": "step9_supervisor",
              "target": "step10_reload_status"
            },
            {
              "label": null,
              "source": "n_1766207472527",
              "target": "n_1766207420852"
            },
            {
              "label": null,
              "source": "n_1766207420852",
              "target": "step3_init_vars"
            },
            {
              "source": "step3_init_vars",
              "target": "step4_eval"
            },
            {
              "label": "true",
              "source": "step5_1_check_retry",
              "target": "step5_2_llm_fix"
            },
            {
              "label": "false",
              "source": "step5_1_check_retry",
              "target": "step6_save_metric"
            },
            {
              "source": "step5_2_llm_fix",
              "target": "step5_3_inc_trial"
            },
            {
              "source": "step5_3_inc_trial",
              "target": "step4_eval"
            },
            {
              "label": null,
              "source": "step10_reload_status",
              "target": "n_1766216815818"
            },
            {
              "label": "false",
              "source": "step8_check_threshold",
              "target": "n_1766216815818"
            },
            {
              "label": null,
              "source": "n_1766217087160",
              "target": "n_1766217292294"
            },
            {
              "label": null,
              "source": "n_1766217292294",
              "target": "sub_end"
            },
            {
              "label": null,
              "source": "step1_save_session",
              "target": "trim_hypothesis"
            },
            {
              "label": null,
              "source": "trim_hypothesis",
              "target": "n_1766207472527"
            },
            {
              "label": null,
              "source": "n_1766216815818",
              "target": "trim_exp_design"
            },
            {
              "label": null,
              "source": "trim_exp_design",
              "target": "n_1766217006428"
            },
            {
              "label": null,
              "source": "n_1766217006428",
              "target": "trim_result_analysis"
            },
            {
              "label": null,
              "source": "trim_result_analysis",
              "target": "n_1766217087160"
            }
          ],
          "nodes": [
            {
              "config": {},
              "id": "sub_start",
              "label": "Sub Start",
              "position": {
                "x": -789,
                "y": 104
              },
              "type": "start"
            },
            {
              "config": {
                "model": "auto-gemini-2.5",
                "response_output": "hypothesis_output",
                "session_id_output": "session_v1",
                "session_mode": "new",
                "timeout": 600,
                "user_template": "Hint from LLM supervisor:\n\uff08\u8fd9\u4e2a\u5efa\u8bae\u7ea7\u522b\u6bd4\u5176\u4ed6\u5efa\u8bae\u90fd\u8981\u9ad8\uff09\n{hint}\n\n{DEFAULT_SYS}\n\n[CRITICAL SAFETY WARNING]\nYou are STRICTLY FORBIDDEN from modifying any files outside the Current Working Directory. Any attempt to modify files in the outside will be detected and reverted immediately.\n\nCurrent Working Directory: {current_exp_path}\nVenv: {venv}\nCycle: {cycle}/{n_cycles}\nPrevious Hypothesis: {hypothesis}\nPrevious Experiment Design: {exp_design}\nPrevious Results: {result_analysis}\nImproved from Parent: {is_improved}\nMore experiment record:\n{lessons_text}\n\nHuman Instructions:\nstep1: Analyze the previous research context. Read the strategy code (*.py) code. Make sure you read the important result picture {plot_names}. They are bad windows in @eval_out.txt.\nstep2: Output a plan to improve the strategy in string format. explain how it relates to ur analysis."
              },
              "id": "step1_impl",
              "label": "1.make hypothesis",
              "position": {
                "x": -466,
                "y": 124
              },
              "type": "llm_generate"
            },
            {
              "config": {
                "key": "gemini_session_id",
                "mode": "overwrite",
                "value_template": "{session_v1}",
                "value_type": "string"
              },
              "id": "step1_save_session",
              "label": "1.1 Save Session ID",
              "position": {
                "x": -498,
                "y": 545
              },
              "type": "write_history"
            },
            {
              "config": {
                "command": "cd {current_exp_path} && export PYTHONPATH=$PYTHONPATH:../../ && {venv} -c \"from evaluator import evaluate; print('Best metric:', evaluate('strategy.py'))\" > eval_out.txt 2>&1; cat eval_out.txt",
                "output_vars": [
                  "test_output"
                ],
                "timeout": 1200
              },
              "id": "step4_eval",
              "label": "4. Run Evaluator",
              "position": {
                "x": 602,
                "y": 535
              },
              "type": "run_shell"
            },
            {
              "config": {
                "code": "\nimport re\nlogger.log(\"\ud83d\udc1b DEBUG: Step 4.5 Parse Metric Started\")\noutput = context.get('test_output', '')\nlogger.log(f'\ud83d\udc1b DEBUG: test_output length: {len(output)}')\nif len(output) < 200: logger.log(f'\ud83d\udc1b DEBUG: test_output content: {output}')\n\nmetric = None\ntry:\n    match = re.search(r\"Best metric:\\s*([\\d\\.]+)\", output)\n    if match:\n        metric = float(match.group(1))\n        logger.log(f'\ud83d\udc1b DEBUG: Matched Best metric: {metric}')\n    else:\n        logger.log(\"\ud83d\udc1b DEBUG: No 'Best metric' found.\")\nexcept Exception as e:\n    logger.log(f'\ud83d\udc1b DEBUG: Regex Error: {e}')\n\nif metric is None:\n    logger.log(\"\u26a0\ufe0f Could not parse metric. Setting current_metric to None.\")\n    context['current_metric'] = None # Explicitly set to None if not found\nelse:\n    context['current_metric'] = metric\n    logger.log(f'\u2705 Metric Parsed and Set: {metric}')\n"
              },
              "id": "step4_5_parse",
              "label": "4.5 Parse Metric",
              "position": {
                "x": 919,
                "y": 522
              },
              "type": "python_script"
            },
            {
              "config": {
                "direction": "min",
                "metric_key": "metric_score",
                "threshold": 0
              },
              "id": "step5_check",
              "label": "5. Check Improvement",
              "position": {
                "x": 1223,
                "y": 584
              },
              "type": "check_improvement"
            },
            {
              "config": {
                "key": "metrics",
                "mode": "overwrite",
                "value_template": "{current_metric}",
                "value_type": "number"
              },
              "id": "step6_save_metric",
              "label": "6. Save Metric",
              "position": {
                "x": 1559,
                "y": 60
              },
              "type": "write_history"
            },
            {
              "config": {
                "key": "if_improved",
                "mode": "overwrite",
                "value_template": "{is_improved}",
                "value_type": "boolean"
              },
              "id": "step7_save_imp",
              "label": "7. Save Improvement",
              "position": {
                "x": 1690,
                "y": 312
              },
              "type": "write_history"
            },
            {
              "config": {
                "code": "improved = context.get('is_improved', False)\npm = context.get('parent_metric')\ncm = context.get('current_metric')\nresult = False\nif improved and pm and cm and isinstance(pm, (int, float)) and isinstance(cm, (int, float)) and pm != 0:\n    imp_pct = (pm - cm) / abs(pm)\n    if imp_pct > 0.05:\n        result = True\n        logger.log(f\"\ud83d\udc6e Supervisor Triggered: Improvement {imp_pct:.2%} > 5%\")"
              },
              "id": "step8_check_threshold",
              "label": "Improvement > 5%?",
              "position": {
                "x": 1914,
                "y": 46
              },
              "type": "condition_code"
            },
            {
              "config": {
                "model": "auto-gemini-2.5",
                "response_output": "supervisor_log",
                "session_id_input": "session_v1",
                "session_mode": "inherit",
                "timeout": 600,
                "user_template": "{DEFAULT_SYS}\nnow we just have a breakthrough, you need to check if this breakthrough is reliable.\ntask1:\nMainly by checking if the images are normal: {plot_names}. If not normal, analyze the reason.\ntask2:\nBased on your analysis, update ./history.json key 'supervisor comment', 'if_improved'. If this breakthrough is achieved by cheating (manipulating evaluation standards or significantly adjusting the number of high/low points), or is not reliable, then you must change 'if_improved'=False."
              },
              "id": "step9_supervisor",
              "label": "Supervisor Check",
              "position": {
                "x": 2168,
                "y": -72
              },
              "type": "llm_generate"
            },
            {
              "config": {
                "code": "import json\ntry:\n    with open(context['current_exp_path'] + '/history.json', 'r') as f:\n        h = json.load(f)\n        if 'if_improved' in h:\n            context['is_improved'] = h['if_improved']\n            logger.log(f\"\ud83d\udc6e Supervisor Status Reload: is_improved={context['is_improved']}\")\nexcept: pass"
              },
              "id": "step10_reload_status",
              "label": "Reload Status",
              "position": {
                "x": 2409,
                "y": 11
              },
              "type": "python_script"
            },
            {
              "config": {},
              "id": "sub_end",
              "label": "Sub End",
              "position": {
                "x": 2929,
                "y": 264
              },
              "type": "end"
            },
            {
              "config": {
                "model": "gemini-2.5-flash",
                "response_output": "llm_response",
                "session_id_input": "session_v1",
                "session_id_output": "session_id",
                "session_mode": "inherit",
                "timeout": 600,
                "user_template": "Hint from LLM supervisor:\n\uff08\u8fd9\u4e2a\u5efa\u8bae\u7ea7\u522b\u6bd4\u5176\u4ed6\u5efa\u8bae\u90fd\u8981\u9ad8\uff09\n{hint}\n\n{DEFAULT_SYS}\n\n[CRITICAL SAFETY WARNING]\nYou are STRICTLY FORBIDDEN from modifying any files outside the Current Working Directory. Any attempt to modify files in the outside will be detected and reverted immediately.\n\nHuman Instructions:\nstep1: Modify strategy code (*.py) to implement ur hypothesis. You are encouraged to add/remove print statements for intermediate variables to clarify the research process. But u should try to reduce the warnning output or repetitive output because they will distract u from important variables.\n\nNote that u not need to run code. I will run export PYTHONPATH=$PYTHONPATH:../../ && {venv} -c \"from evaluator import evaluate; print('Best metric:', evaluate('strategy.py'))\" > eval_out.txt 2>&1; cat eval_out.txt later\n"
              },
              "id": "n_1766207420852",
              "label": "2. implement",
              "position": {
                "x": -159,
                "y": 123
              },
              "type": "llm_generate"
            },
            {
              "config": {
                "key": "hypothesis",
                "mode": "overwrite",
                "value_template": "{hypothesis_output}",
                "value_type": "string"
              },
              "id": "n_1766207472527",
              "label": "1.2 save hypothesis",
              "position": {
                "x": -26,
                "y": 549
              },
              "type": "write_history"
            },
            {
              "config": {
                "code": "context['correction_trials'] = 0"
              },
              "id": "step3_init_vars",
              "label": "3. Init Correction Vars",
              "position": {
                "x": 377,
                "y": 228
              },
              "type": "python_script"
            },
            {
              "config": {
                "code": "result = (not context.get('is_improved', False)) and (context.get('correction_trials', 0) < 3)"
              },
              "id": "step5_1_check_retry",
              "label": "5.1 Retry?",
              "position": {
                "x": 1272,
                "y": 173
              },
              "type": "condition_code"
            },
            {
              "config": {
                "model": "gemini-2.5-flash",
                "response_output": "correction_log",
                "session_id_input": "session_v1",
                "session_mode": "inherit",
                "timeout": 600,
                "user_template": "{DEFAULT_SYS}\n\nThe previous attempt failed to improve the metric.\nCurrent Metric: {current_metric}\nParent Metric: {parent_metric}\nCorrection Trial: {correction_trials}/5\n\nEvaluator Output:\n{test_output}\n\nTask:\nAnalyze the failure and the evaluator output.\nModify 'strategy.py' to fix the issue or try a different approach to improve the metric.\nEnsure the code is valid and runnable."
              },
              "flip": true,
              "id": "step5_2_llm_fix",
              "label": "5.2 LLM Correction",
              "position": {
                "x": 987,
                "y": 178
              },
              "type": "llm_generate"
            },
            {
              "config": {
                "code": "context['correction_trials'] = context.get('correction_trials', 0) + 1"
              },
              "flip": true,
              "id": "step5_3_inc_trial",
              "label": "5.3 Inc Trial",
              "position": {
                "x": 740,
                "y": 182
              },
              "type": "python_script"
            },
            {
              "config": {
                "model": "gemini-2.5-flash-lite",
                "session_id_input": "session_v1",
                "session_id_output": "session_id",
                "session_mode": "inherit",
                "timeout": 600,
                "user_template": "{DEFAULT_SYS}\n\nEvaluator Output:\n{test_output}\n\n[CRITICAL SAFETY WARNING]\nYou are STRICTLY FORBIDDEN from modifying any files outside the Current Working Directory. Any attempt to modify files in the outside will be detected and reverted immediately.\n\nHuman Instructions:\nstep1: Analyze current research context. Understand what experiment design is adopted this round.\nstep2: Output a 'experiment design' to summarize it in string format."
              },
              "id": "n_1766216815818",
              "label": "experiment design",
              "position": {
                "x": 2185,
                "y": 371
              },
              "type": "llm_generate"
            },
            {
              "config": {
                "model": "gemini-2.5-flash-lite",
                "response_output": "result_analysis",
                "session_id_input": "session_v1",
                "session_id_output": "session_id",
                "session_mode": "inherit",
                "timeout": 600,
                "user_template": "{DEFAULT_SYS}\n\nEvaluator Output:\n{test_output}\n[CRITICAL SAFETY WARNING]\nYou are STRICTLY FORBIDDEN from modifying any files outside the Current Working Directory. Any attempt to modify files in the outside will be detected and reverted immediately.\n\nHuman Instructions:\nstep1: Analyze current research context. Do a result analysis of  this round.\nstep2: Output a 'result analysis' to summarize it in string format."
              },
              "id": "n_1766217006428",
              "label": "result analysis",
              "position": {
                "x": 2459,
                "y": 391
              },
              "type": "llm_generate"
            },
            {
              "config": {
                "key": "exp_design",
                "mode": "overwrite",
                "value_template": "{experiment_design}",
                "value_type": "string"
              },
              "id": "n_1766217087160",
              "label": "write exp_design",
              "position": {
                "x": 2696,
                "y": 467
              },
              "type": "write_history"
            },
            {
              "config": {
                "key": "result_analysis",
                "mode": "overwrite",
                "value_template": "{result_analysis}",
                "value_type": "string"
              },
              "id": "n_1766217292294",
              "label": "write result_analysis",
              "position": {
                "x": 2864,
                "y": 565
              },
              "type": "write_history"
            },
            {
              "config": {
                "code": "val = context.get('hypothesis_output', '')\nstr_len = 6000\nhalf_len = 3000\nif isinstance(val, str) and len(val) > str_len:\n    context['hypothesis_output'] = val[:half_len] + '...[TRUNCATED]...' + val[-half_len:]\n    logger.log(f\"Trimmed 'hypothesis_output' from {len(val)} to {len(context['hypothesis_output'])} chars\")\n"
              },
              "id": "trim_hypothesis",
              "label": "Trim Hypothesis",
              "position": {
                "x": -254,
                "y": 551
              },
              "type": "python_script"
            },
            {
              "config": {
                "code": "val = context.get('experiment_design', '')\nstr_len = 6000\nhalf_len = 3000\nif isinstance(val, str) and len(val) > str_len:\n    context['experiment_design'] = val[:half_len] + '\\n...[TRUNCATED]...\\n' + val[-half_len:]\n    logger.log(f\"Trimmed 'experiment_design' from {len(val)} to {len(context['experiment_design'])} chars\")"
              },
              "id": "trim_exp_design",
              "label": "Trim Exp Design",
              "position": {
                "x": 2174,
                "y": 626
              },
              "type": "python_script"
            },
            {
              "config": {
                "code": "val = context.get('result_analysis', '')\nstr_len = 6000\nhalf_len = 3000\nif isinstance(val, str) and len(val) > str_len:\n    context['result_analysis'] = val[:half_len] + '\\n...[TRUNCATED]...\\n' + val[-half_len:]\n    logger.log(f\"Trimmed 'result_analysis' from {len(val)} to {len(context['result_analysis'])} chars\")"
              },
              "id": "trim_result_analysis",
              "label": "Trim Result Analysis",
              "position": {
                "x": 2484,
                "y": 771
              },
              "type": "python_script"
            }
          ]
        }
      },
      "id": "subloop_node",
      "label": "Experiment Subloop",
      "position": {
        "x": 546,
        "y": 368
      },
      "type": "subloop"
    },
    {
      "config": {
        "code": "result = context.get('is_improved', False)"
      },
      "id": "check_imp",
      "label": "Check Improvement",
      "position": {
        "x": 800,
        "y": 300
      },
      "type": "condition_code"
    },
    {
      "config": {
        "code": "# IMPROVED: Go to next layer\ncontext['layer_idx'] += 1\ncontext['sub_idx'] = 1\n# Parent is the current successful experiment\ncontext['next_parent_path'] = context['current_exp_path']\n# Clear lessons as we start fresh layer\ncontext['lessons_text'] = ''\n# Keep parent_metric for next check\ncontext['parent_metric'] = context.get('current_metric')\nlogger.log(f'\u2705 Improved! Moving to Layer {context[\"layer_idx\"]}')\ncontext['consecutive_failures'] = 0"
      },
      "id": "next_layer",
      "label": "Next Layer (L+1, S=1)",
      "position": {
        "x": 1000,
        "y": 150
      },
      "type": "python_script"
    },
    {
      "config": {
        "filter": "Failures Only",
        "lookback_count": 5,
        "offset": 1,
        "output_var": "lessons_text",
        "scope": "Same Branch/Layer"
      },
      "id": "lesson_node",
      "label": "Collect Lesson",
      "position": {
        "x": 1000,
        "y": 450
      },
      "type": "lesson"
    },
    {
      "config": {
        "code": "# FAILED: Retry current layer\ncontext['sub_idx'] += 1\npass # Logic handled by Setup Workspace defaults or existing context state\nlogger.log(f'\u274c Failed. Retrying Layer {context[\"layer_idx\"]} (Sub {context[\"sub_idx\"]})')"
      },
      "id": "next_attempt",
      "label": "Next Attempt (S+1)",
      "position": {
        "x": 1200,
        "y": 450
      },
      "type": "python_script"
    },
    {
      "config": {
        "code": "result = context['cycle'] < context.get('n_cycles', 10)"
      },
      "id": "check_cycles",
      "label": "Check Cycles",
      "position": {
        "x": 1400,
        "y": 300
      },
      "type": "condition_code"
    },
    {
      "config": {},
      "id": "end",
      "label": "End",
      "position": {
        "x": 1600,
        "y": 450
      },
      "type": "end"
    },
    {
      "config": {
        "code": "context['consecutive_failures'] = context.get('consecutive_failures', 0) + 1\nlogger.log(f'\u26a0\ufe0f Consecutive Failures: {context[\"consecutive_failures\"]}')"
      },
      "id": "increment_failures",
      "label": "Increment Failures",
      "position": {
        "x": 800,
        "y": 450
      },
      "type": "python_script"
    },
    {
      "config": {
        "code": "result = context['consecutive_failures'] >= 3"
      },
      "id": "check_fail_threshold",
      "label": "Fail >= 5?",
      "position": {
        "x": 564,
        "y": 672
      },
      "type": "condition_code"
    },
    {
      "config": {
        "code": "target_path = context.get('next_parent_path')\nif not target_path:\n    try:\n        cur = Path(context['current_exp_path'])\n        with open(cur / 'history.json') as f:\n             h = json.load(f)\n             p = h.get('parent_exp')\n             if p and 'Branch' in p:\n                 target_path = service.tasks_dir / p\n             elif p == 'example_workspace':\n                 target_path = service.tasks_dir / 'Branch_example' / 'exp_example'\n    except: pass\n\nif target_path:\n     # CRITICAL FIX: Ensure target_path is a Path object, not a string\n     target_path = Path(target_path)\n     context['current_exp_path'] = str(target_path)\n     logger.log(f'\ud83d\udd04 Meta-Analysis: Switched context to {target_path}')\n     \n     source_exp_id = f\"exp{context.get('branch_idx', 0)}.{context.get('layer_idx', 0)}.{context.get('sub_idx', 0)}\"\n     \n     try:\n        h_path = target_path / 'history.json'\n        if h_path.exists():\n            with open(h_path, 'r') as f:\n                h = json.load(f)\n            \n            hint_val = h.get('hint', '')\n            context['hint'] = hint_val\n            \n            sess_val = h.get('gemini_session_id', '')\n            logger.log(f\"\ud83d\udd0e Reading Session from {h_path.name}: '{sess_val}'\")\n            \n            if sess_val:\n                context['gemini_session_id'] = sess_val\n            else:\n                logger.log(\"\u26a0\ufe0f Warning: Parent history has no 'gemini_session_id'. Meta-Analysis will start new session.\")\n                context['gemini_session_id'] = '' \n            \n            h['hint_from'] = source_exp_id\n            \n            with open(h_path, 'w') as f:\n                json.dump(h, f, indent=2, ensure_ascii=False)\n            \n            logger.log(f\"\ud83d\udcdd Updated 'hint_from' in parent history to {source_exp_id}\")\n            \n     except Exception as e:\n        logger.error(f\"Failed to update parent history: {e}\")\nelse:\n     logger.log('\u26a0\ufe0f Meta-Analysis: No success found. Staying in current.')"
      },
      "id": "restore_last_success",
      "label": "Restore Last Success",
      "position": {
        "x": 836,
        "y": 673
      },
      "type": "python_script"
    },
    {
      "config": {
        "filter": "Failures Only",
        "lookback_count": 10,
        "offset": 1,
        "output_var": "meta_lessons",
        "scope": "All History"
      },
      "id": "meta_lesson",
      "label": "Hint Lesson",
      "position": {
        "x": 1065,
        "y": 672
      },
      "type": "lesson"
    },
    {
                    "config": {
                      "model": "auto-gemini-2.5",        "response_output": "hint_output",
        "session_id_input": "gemini_session_id",
        "session_mode": "inherit",
        "timeout": 1200,
        "user_template": "{DEFAULT_SYS}\nPrevious experiment records:\n{meta_lessons}\nPrevious web hint:\n{hint}\nresult plot: {plot_names}\n\nHuman: now, the current experiments have failed to achieve a breakthrough. Analyze the code in the current working directory and the history. Conduct a web search with your questions, making sure to use your networking tools. Optional sources include but are not limited to:\n1. View papers on Arxiv (HTML or PDF). Or use webfetch/GoogleSearch. Explain the inspiration combining with the original text.\n2. Look for discussions in online communities.\n3. Check GitHub for relevant repositories and explain the inspiration combining with the repository code.\n\nTask:\n1. Do detailed analysis and web research. \n2. Write a brainstorm text to guide future LLM experiments, ur plan must have a short reference to real online source (paper text,online discussion,or github original code) so that I can trace back. Do not repeat previous ideas.\n"
      },
      "id": "meta_analysis_llm",
      "label": "Hint generation LLM",
      "position": {
        "x": 1313,
        "y": 659
      },
      "type": "llm_generate"
    },
    {
      "config": {
        "code": "context['consecutive_failures'] = 0\nlogger.log('\ud83d\udd04 Fail count reset after Meta-Analysis.')"
      },
      "id": "reset_fail_count",
      "label": "Reset Fail Count",
      "position": {
        "x": 1589,
        "y": 643
      },
      "type": "python_script"
    },
    {
      "id": "n_1766633725413",
      "type": "python_script",
      "label": "Trim hint",
      "position": {
        "x": 1315,
        "y": 906
      },
      "config": {
        "code": "val = context.get('hint_output', '')\nstr_len = 6000\nhalf_len = 3000\nif isinstance(val, str) and len(val) > str_len:\n    context['hint_output'] = val[:half_len] + '...[TRUNCATED]...' + val[-half_len:]\n    logger.log(f\"Trimmed 'hint_output' from {len(val)} to {len(context['hint_output'])} chars\")\n"
      }
    },
    {
      "id": "n_1766633914418",
      "type": "write_history",
      "label": "write hint",
      "position": {
        "x": 1542,
        "y": 919
      },
      "config": {
        "key": "hint",
        "mode": "overwrite",
        "value_type": "string",
        "value_template": "{hint_output}"
      }
    }
  ]
}