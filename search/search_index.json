{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AgentCommander","text":"<p>Automating the loop of discovery.</p> <p></p> <p> </p> <p>AgentCommander was born from the actual demands of scientific research.</p> <p>Refined through rigorous practical application, it is a graph-based workflow engine designed to orchestrate AI Agents for complex, iterative tasks. Built to leverage the diverse ecosystem of LLM CLIs (Gemini, Qwen, Claude, etc.), it enables Machine Learning engineers to construct highly customizable, infinite-loop workflows.</p>"},{"location":"#philosophy-human-centric-evolution","title":"Philosophy: Human-Centric Evolution","text":"<p>Unlike \"set and forget\" evolutionary algorithms (like AlphaEvolve or OpenEvolve) where agents are left to mutate randomly in a vacuum, AgentCommander is built for Collaborative Discovery.</p> <ul> <li>You are the Commander: You define the search space, the hypothesis generation logic, and the evaluation metrics via a visual workflow.</li> <li>AI is the Executor: The agent handles the exhaustive, repetitive loop of coding, debugging, and refining.</li> <li>Transparent &amp; Controllable: We prioritize white-box transparency. Every step is logged, every file is accessible, and you can intervene at any moment.</li> </ul>"},{"location":"#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Symbolic Regression: Automating the search for mathematical expressions.</li> <li>ML Structure/Hyperparameter Optimization: Intelligent tuning without manual intervention.</li> <li>Autonomous Model Refinement: Self-correcting loops for model improvement.</li> </ul>"},{"location":"#contact","title":"Contact","text":"<p>\ud83d\udce7 Email: miaoxin.liu@u.nus.edu</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ol> <li>Python 3.10+</li> <li>LLM CLI Tool (One of the following):<ul> <li>Gemini CLI (Recommended): <code>npm install -g @google/gemini-cli@latest</code><ul> <li>Tip: Enable \"Gemini Preview\" in settings to access Pro 3/Flash 3 models.</li> </ul> </li> <li>Qwen CLI: <code>npm install -g @qwen/cli</code></li> <li>OpenCode AI: <code>npm install -g opencode-ai</code></li> <li>Claude Code: <code>npm install -g @anthropic-ai/claude-code</code></li> </ul> </li> </ol>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#os-support","title":"OS Support","text":"<ul> <li>\u2705 Linux &amp; macOS: Fully supported (native).</li> <li>\u26a0\ufe0f Windows: Highly recommended to use WSL2.</li> </ul>"},{"location":"getting-started/#steps","title":"Steps","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/yourusername/AgentCommander.git\ncd AgentCommander\n</code></pre></p> </li> <li> <p>Install dependencies: <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Start the UI Server: <pre><code>bash run_ui.sh\n</code></pre>     This will start the web server at <code>http://localhost:8080</code>.</p> </li> </ol> <p></p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that the server is running, you have two paths:</p> <ol> <li>Initialize a New Project: Use the Auto-Setup Wizard to scaffold your environment.<ul> <li>\ud83d\udc49 See \"Project Setup\" &gt; \"Auto-Setup Wizard\".</li> </ul> </li> <li>Run a Demo: Try the included Diabetes example to see the agent in action immediately.<ul> <li>\ud83d\udc49 See \"Examples &amp; Guides\" &gt; \"Diabetes Example\".</li> </ul> </li> </ol> <p>For advanced configuration (e.g., enabling Gemini Pro), check \"Reference &amp; Tips\" &gt; \"CLI Guide\".</p>"},{"location":"automated-engine/","title":"Agent Capability","text":"<p>AgentCommander empowers AI agents to mimic the cognitive loop of a human researcher: Hypothesize, Observe, Reference, and Summarize.</p> <p>By orchestrating these cognitive steps into a structured workflow, we achieve a highly efficient, automated trial-and-error engine.</p>"},{"location":"automated-engine/#the-default-research-cycle","title":"The Default Research Cycle","text":"<p>In the default configuration (<code>default_graph.json</code>), every time a new experiment folder is created, the Agent follows a rigorous scientific process:</p>"},{"location":"automated-engine/#1-hypothesis-generation","title":"1. Hypothesis Generation","text":"<ul> <li>Context: The agent reads the current code, previous execution logs, and critically, performs Visual Analysis on any result plots (e.g., loss curves, prediction scatter plots).</li> <li>Action: Based on this multimodal input, it formulates a specific hypothesis for the current experiment (e.g., \"The model is overfitting, so I will increase dropout\").</li> </ul>"},{"location":"automated-engine/#2-implementation-debugging","title":"2. Implementation &amp; Debugging","text":"<ul> <li>Coding: The agent modifies the code (<code>strategy.py</code>) to test the hypothesis.</li> <li>Observability: It proactively adds debug print statements to gather more information during execution, just like a human developer would.</li> <li>Execution: The code is run against the immutable <code>evaluator.py</code>.</li> </ul>"},{"location":"automated-engine/#3-summary-history","title":"3. Summary &amp; History","text":"<ul> <li>Record: Regardless of success or failure, the agent summarizes the experiment design and performance into <code>history.json</code>.</li> <li>Evolution: If the score improves, it advances to the next level. If it fails, it retries until a limit is reached.</li> </ul>"},{"location":"automated-engine/#4-meta-reflection-getting-unstuck","title":"4. Meta-Reflection (Getting Unstuck)","text":"<ul> <li>Memory: When starting a new experiment, the agent reads the history of previous attempts.</li> <li>External Inspiration: If the system detects a stagnation (multiple generations without breakthrough), it triggers a special Meta-Analysis step. The agent uses Online Search tools to find relevant papers (Arxiv) or open-source repositories (GitHub) to find fresh inspiration.</li> </ul>"},{"location":"automated-engine/evolution-strategy/","title":"Evolutionary Strategy","text":"<p>AgentCommander manages the lifecycle of experiments using a structured evolution logic.</p>"},{"location":"automated-engine/evolution-strategy/#default-strategy-bls-branchlevelstep","title":"Default Strategy: B/L/S (Branch/Level/Step)","text":"<p>The default workflow (<code>default_graph.json</code>) implements a heuristic evolutionary tree:</p> <ul> <li>Branch (B): Represents major conceptual directions or distinct lineages.</li> <li>Level (L): Represents depth of optimization. When an experiment succeeds (improves the metric), the system spawns a new generation at Level + 1, inheriting the successful traits.</li> <li>Step (S): Represents horizontal trial-and-error. When an experiment fails, the system retries at Step + 1 with a modified hypothesis, keeping the Level constant.</li> </ul>"},{"location":"automated-engine/evolution-strategy/#customization-parallelism","title":"Customization &amp; Parallelism","text":"<p>It is important to note that B/L/S is simply the default template.</p>"},{"location":"automated-engine/evolution-strategy/#flexible-graph-architecture","title":"Flexible Graph Architecture","text":"<p>The entire logic is defined in the visual workflow editor. You are free to redesign this graph to implement: *   Genetic Algorithms: Selection, Crossover, Mutation nodes. *   Bayesian Optimization: Logic nodes that update hyperparameters based on past results.</p>"},{"location":"automated-engine/evolution-strategy/#parallel-exploration","title":"Parallel Exploration","text":"<p>The underlying engine supports Parallel Execution. You can design workflows where multiple agents explore different branches simultaneously, or where a \"Manager Agent\" spawns multiple \"Worker Agents\" to solve sub-problems in parallel. This scalability allows AgentCommander to leverage massive compute resources for accelerated discovery.</p>"},{"location":"automated-engine/inner-loop/","title":"Inner Loop Logic","text":"<p>The Experiment Subloop encapsulates the scientific method.</p>"},{"location":"automated-engine/inner-loop/#standard-flow","title":"Standard Flow","text":"<ol> <li>Hypothesis Generation: Review history and propose a new idea.</li> <li>Implementation: Write/Modify <code>strategy.py</code>.</li> <li>Evaluation: Run <code>evaluator.py</code>.</li> <li>Analysis: Parse logs, update history, and decide if the attempt was successful.</li> </ol>"},{"location":"automated-engine/safety-sandbox/","title":"Security &amp; Sandbox System","text":"<p>AgentCommander addresses the safety challenges of autonomous agents with a multi-layer \"Soft Sandbox\" built on top of the filesystem.</p>"},{"location":"automated-engine/safety-sandbox/#directory-level-sandboxing","title":"Directory-Level Sandboxing","text":"<p>Unlike other agent frameworks where file system isolation can be difficult to enforce, the CLI-based approach allows AgentCommander to strictly limit the agent's read/write access to specific experiment directories. This ensures that the agent can freely experiment within its sandbox without risking modifications to your system-level files.</p> <ul> <li>Isolation: The CLI tools focus on files within the specified working directory (<code>root_dir</code>). </li> <li>Transparent Debugging (White-box): Every experiment runs in its own folder. You can simply <code>cd</code> into any experiment directory and run <code>gemini -r</code>, <code>qwen -c</code>, or <code>opencode -c</code> to resume the conversation. This allows you to inspect the final formatted prompt, review the history, and manually intervene.</li> </ul>"},{"location":"automated-engine/safety-sandbox/#file-permission-modes-llm-nodes","title":"File Permission Modes (LLM Nodes)","text":"<p>To prevent LLM agents from modifying unauthorized files, AgentCommander implements a strict File Permission System for each <code>llm_generate</code> node:</p> <ul> <li>Strict (Read-Only): The LLM is strictly forbidden from modifying existing files.</li> <li>Restricted (Whitelist/Blacklist): The LLM can only modify the files or folders explicitly listed (e.g., <code>strategy.py</code>).</li> <li>Open (Allow All): Unrestricted access within the working directory.</li> </ul> <p>Enforcement: The system creates a filesystem snapshot before execution and compares it after. Any unauthorized changes are immediately reverted.</p>"},{"location":"automated-engine/safety-sandbox/#no-exec-mechanism-execution-ban","title":"No-Exec Mechanism (Execution Ban)","text":"<p>To prevent agents from accidentally (or maliciously) executing unfinished or dangerous code during the generation phase, we implement a strict No-Exec Lock:</p> <ul> <li>Behavior: Before the LLM generates code, the system automatically runs <code>chmod -x</code> on critical files (e.g., <code>evaluator.py</code>, <code>strategy.py</code>) listed in the <code>no_exec_files</code> configuration of the node.</li> <li>Prompt Injection: The system prompt is automatically updated to warn the AI: \"Do NOT try to run the code yourself.\"</li> <li>Enforcement: Even if the AI ignores the prompt and tries to run <code>./strategy.py</code>, the Operating System will deny permission.</li> </ul>"},{"location":"automated-engine/safety-sandbox/#2-filesystem-snapshot-rollback","title":"2. Filesystem Snapshot &amp; Rollback","text":"<p>Before any AI node executes, AgentCommander takes a \"snapshot\" of the current directory.</p> <ul> <li>Strict Mode: Any file modification not explicitly allowed is reverted.</li> <li>Whitelist/Blacklist: You can define granular rules (e.g., \"Allow editing <code>strategy.py</code>, but forbid editing <code>evaluator.py</code>\").</li> <li>Smart Ignoring: The system intelligently ignores noise files like <code>__pycache__</code> to prevent false positive security warnings.</li> </ul>"},{"location":"automated-engine/safety-sandbox/#3-parent-directory-lock","title":"3. Parent Directory Lock","text":"<p>When <code>lock_parent</code> is enabled, the system temporarily removes write permissions (<code>chmod u-w</code>) from the parent directory of the experiment. This prevents the agent from \"escaping\" its sandbox and modifying sibling experiments or system files.</p>"},{"location":"automated-engine/safety-sandbox/#4-cli-permissions-yolo-mode","title":"4. CLI Permissions (YOLO Mode)","text":"<p>By default, the underlying CLI tools operate in YOLO mode (e.g., using <code>yolo</code> or <code>skip-dangerous-permission</code> flags). This grants the model permission to use available tools autonomously within the working directory.</p>"},{"location":"automated-engine/safety-sandbox/#risks","title":"Risks","text":"<ul> <li>Arbitrary Privilege: While AgentCommander restricts filesystem access via its sandbox, the agent still inherits the permissions of the OS user running the process.</li> <li>Prompt Injection: Malicious or poorly designed prompts could theoretically guide an agent to perform unintended actions.</li> </ul>"},{"location":"automated-engine/safety-sandbox/#disclaimer","title":"Disclaimer","text":"<p>This software is provided \"as is\", without warranty of any kind. The developers are not responsible for any damage, loss of data, or security breaches. </p> <p>Recommendation: Always run AgentCommander inside a Docker Container, Virtual Machine, or a dedicated restricted user account to ensure complete isolation.</p>"},{"location":"guides/diabetes-example/","title":"Diabetes Classification Example","text":"<p>This example demonstrates optimizing a Scikit-Learn model.</p>"},{"location":"guides/diabetes-example/#quick-start-guide","title":"Quick Start Guide","text":"<p>You can verify your installation immediately by running our pre-configured Scikit-Learn example without any setup.</p> <ol> <li>Open Control Panel: In the UI, stay on the Control tab.</li> <li>Set Root Directory: Set the <code>Root Directory</code> field to <code>example/diabetes_sklearn</code>.</li> <li>Configure <code>eval_cmd</code>:<ul> <li>In the Global Variables list, find <code>eval_cmd</code>.</li> <li>Ensure it is set to: <code>python evaluator.py</code> (ensure your environment's python is used).</li> </ul> </li> <li>Save &amp; Start: Click \ud83d\udcbe Save Config, then click \u25b6 Start Agent.<ul> <li>The agent will immediately start its first cycle, analyzing the diabetes data and generating model hypotheses.</li> </ul> </li> </ol>"},{"location":"guides/diabetes-example/#location","title":"Location","text":"<p><code>example/diabetes_sklearn</code></p>"},{"location":"guides/diabetes-example/#reference-experiment-structure","title":"Reference Experiment Structure","text":"<p>The <code>example/diabetes_sklearn/</code> directory provides a reference implementation for organizing machine learning experiments. It is designed to be robust and prevent \"cheating\" by the LLM.</p>"},{"location":"guides/diabetes-example/#file-organization","title":"File Organization","text":"<ul> <li>Evaluation Script (<code>evaluator.py</code>): The ground truth for assessment. Includes built-in anti-leakage checks and enforces time limits.</li> <li>Seed Experiment Directory: Contains the initial <code>strategy.py</code> and auxiliary files.</li> <li>Experiment History (<code>history.json</code>): Automatically stores metrics and optimal results.</li> </ul>"},{"location":"guides/diabetes-example/#evolutionary-progress-logic-b-l-s","title":"Evolutionary Progress Logic (B, L, S)","text":"<p>The default workflow uses a Branch (B), Level (L), Step (S) logic: *   Branch (B): A new conceptual direction. *   Level (L): Progression in depth (advanced on significant improvement). *   Step (S): Explorations within the same level (retries/mutations).</p>"},{"location":"guides/diabetes-example/#structure","title":"Structure","text":"<ul> <li><code>strategy.py</code>: Defines the Model and Hyperparameter Space.</li> <li><code>evaluator.py</code>: Runs Grid Search and reports MSE.</li> </ul>"},{"location":"guides/hpc-cluster/","title":"HPC Cluster Deployment (PBS)","text":"<p>Run AgentCommander on a supercomputer head node.</p>"},{"location":"guides/hpc-cluster/#templates","title":"Templates","text":"<p>Located in <code>example/pbs_server_example_tasks</code>. *   <code>run_all.pbs</code>: Job submission script. *   <code>watch_job.sh</code>: Wrapper to submit and poll until completion.</p>"},{"location":"guides/hpc-cluster/#usage","title":"Usage","text":"<p>Update <code>eval_cmd</code> in <code>config.json</code> to point to <code>watch_job.sh</code> instead of running python directly.</p>"},{"location":"human-supervision/evolution-tree/","title":"Evolutionary Tree Visualization","text":"<p>Tracking hundreds of experiments is impossible with just a console log. AgentCommander provides a real-time Progress Tree to visualize the lineage of your agents.</p>"},{"location":"human-supervision/evolution-tree/#the-progress-tree","title":"The Progress Tree","text":"<p>Located in the Control Panel, this graph dynamically updates as the agent explores.</p> <p></p>"},{"location":"human-supervision/evolution-tree/#visual-guide","title":"Visual Guide","text":"<ul> <li>Nodes: Each node represents a unique experiment folder (e.g., <code>exp1.2.1</code>).</li> <li>Edges: Directed lines show the inheritance path (Parent $\\to$ Child).</li> <li>Color Coding:<ul> <li>\ud83d\udfe2 Green: Success (Metric improved). Spawns a new Level.</li> <li>\ud83d\udd34 Red: Failure (Metric worsened or crashed). Spawns a retry Step.</li> <li>\ud83d\udd35 Blue: Active (Currently running).</li> </ul> </li> </ul>"},{"location":"human-supervision/evolution-tree/#interaction","title":"Interaction","text":"<ul> <li>Hover: See quick metrics (Score, Hypothesis summary).</li> <li>Click: (Planned Feature) Navigate directly to the file explorer for that experiment.</li> </ul>"},{"location":"human-supervision/file-management/","title":"File Management &amp; Structure","text":"<p>AgentCommander adopts a \"File-First\" architecture. Unlike systems that hide data in databases, we enforce a strict, human-readable directory structure.</p>"},{"location":"human-supervision/file-management/#the-bls-structure","title":"The B.L.S Structure","text":"<p>Experiments are organized hierarchically to make manual navigation intuitive.</p> <pre><code>Project_Root/\n\u251c\u2500\u2500 Branch1/              # Major conceptual direction (e.g., \"CNN Model\")\n\u2502   \u251c\u2500\u2500 exp1.1.1/         # The Seed Experiment\n\u2502   \u251c\u2500\u2500 exp1.2.1/         # Level 2 (Successor of 1.1.1)\n\u2502   \u2514\u2500\u2500 exp1.2.2/         # Step 2 (Retry of 1.2.1 after failure)\n\u2514\u2500\u2500 Branch2/              # New direction (e.g., \"Transformer Model\")\n    \u2514\u2500\u2500 exp2.1.1/\n</code></pre>"},{"location":"human-supervision/file-management/#naming-convention","title":"Naming Convention","text":"<ul> <li>Branch (B): Distinct lineages of evolution.</li> <li>Level (L): Evolutionary depth. Increments on success.</li> <li>Step (S): Trial count within a level. Increments on failure.</li> </ul>"},{"location":"human-supervision/file-management/#integrated-file-explorer","title":"Integrated File Explorer","text":"<p>The Explorer Tab in the UI allows you to navigate through the B.L.S structure and inspect the contents of any experiment folder directly.</p>"},{"location":"human-supervision/file-management/#exploring-experiment-artifacts","title":"Exploring Experiment Artifacts","text":"<p>By clicking into an experiment directory (e.g., <code>exp1.2.1</code>), you can access several critical artifacts:</p> <ul> <li>Evaluation Logs (<code>eval_out.txt</code>): View the raw output of your evaluation script to see performance metrics or debug runtime errors.</li> <li>AI Records (<code>history.json</code>): This file contains the complete cognitive record of the agent for that experiment, including its hypothesis, experiment design, and result analysis.</li> <li>Strategy Code (<code>strategy.py</code>): Inspect the actual code generated or modified by the agent.</li> <li>Visual Results (<code>*.png</code>): View plots and charts generated during evaluation (e.g., <code>best_result.png</code>) to perform your own visual verification.</li> </ul>"},{"location":"human-supervision/file-management/#ai-integration-in-explorer","title":"AI Integration in Explorer","text":"<ul> <li>Context-Aware Chat: An embedded AI assistant that treats the currently open folder as its working directory. You can ask questions like \"Summarize the error logs in this folder.\"</li> <li>Read-Only Mode: A safety toggle for the AI Chat. When enabled, the AI can read and analyze files but is strictly forbidden from modifying or creating files.</li> <li>Quick Actions: Standard file operations including copying paths, renaming, or deleting files manually.</li> </ul>"},{"location":"human-supervision/visual-feedback/","title":"Visual Feedback Loop","text":"<p>AgentCommander supports multimodal analysis.</p>"},{"location":"human-supervision/visual-feedback/#mechanism","title":"Mechanism","text":"<ol> <li>Generate: Your code generates a plot (e.g., <code>plot.png</code>).</li> <li>Feed Back: In the workflow, reference the file using <code>@plot.png</code> in the prompt template.</li> <li>Analyze: Vision-capable models (Gemini Pro Vision, Qwen-VL) will see the image and analyze metrics like overfitting or bias.</li> </ol>"},{"location":"human-supervision/white-box-debugging/","title":"White-box Debugging","text":"<p>AgentCommander adopts a \"White-box\" philosophy. Unlike frameworks that hide agent logic inside opaque execution environments, we allow you to \"step into\" the agent's mind at any time.</p>"},{"location":"human-supervision/white-box-debugging/#pro-tip-resuming-context","title":"\ud83d\udca1 Pro Tip: Resuming Context","text":"<p>Since each experiment node runs in its own isolated directory, you can leverage the persistence features of the CLI tools for deep debugging.</p>"},{"location":"human-supervision/white-box-debugging/#why-is-this-powerful","title":"Why is this powerful?","text":"<ul> <li>Inspect Final Prompts: See exactly what the CLI received after all template variables (like <code>{{hint}}</code> or <code>{{parent_metric}}</code>) were resolved. This is crucial for verifying your prompt engineering logic.</li> <li>Review History: Access the full conversation log (e.g., <code>GEMINI.md</code>) to understand why the agent made a specific decision or error.</li> <li>Manual Intervention: If an agent gets stuck in a loop, you can intervene manually and then let the workflow continue.</li> </ul>"},{"location":"human-supervision/white-box-debugging/#how-to-do-it-gemini-example","title":"How to do it (Gemini Example):","text":"<ol> <li>Open Terminal.</li> <li>Navigate to the experiment folder (e.g., <code>cd my_project/Branch1/exp1.2.1</code>).</li> <li>Run <code>gemini -r</code> (Resume mode).</li> <li>Interact:<ul> <li>Debug Prompt: Type \"Show me the last prompt you received.\" to verify the input context.</li> <li>Analyze Logic: Ask \"Why did you choose this architecture?\"</li> <li>Manual Fix: Directly instruct \"Fix the syntax error on line 45\" to help it recover.</li> </ul> </li> </ol> <p>For Qwen CLI or OpenCode AI users, use <code>qwen -c</code> or <code>opencode -c</code> to achieve the same effect.</p>"},{"location":"human-supervision/workflow-design/","title":"Workflow Design","text":"<p>The Workflow Editor is the command center of AgentCommander. It allows you to visually design the logic flow of your AI agents.</p> <p></p>"},{"location":"human-supervision/workflow-design/#key-concepts","title":"Key Concepts","text":"<ul> <li>Nodes: Individual steps (e.g., LLM Generation, Python Script, Shell Command).</li> <li>Edges: The flow of execution.</li> <li>Subloops: Encapsulated logic blocks (like the Experiment Subloop) that simplify the main view.</li> </ul>"},{"location":"human-supervision/workflow-design/#ai-assistant","title":"AI Assistant","text":"<p>Click the 'AI Assistant' button to modify the graph using natural language (e.g., 'Add a check after step 2').</p>"},{"location":"protocols/auto-setup/","title":"Auto-Setup Wizard","text":"<p>The Auto-Setup Wizard is the fastest way to initialize a robust, AgentCommander-compatible experiment environment.</p>"},{"location":"protocols/auto-setup/#using-the-ui-wizard-recommended","title":"Using the UI Wizard (Recommended)","text":"<p>The easiest way to start is using the built-in Experiment Setup wizard directly in the web UI.</p> <p></p> <ol> <li>Navigate to \"Setup\": Click the \"Experiment Setup\" tab in the UI sidebar.</li> <li>Select a Template:<ul> <li><code>[Case: You only have Dataset]</code>: Corresponds to Scenario 1 below.</li> <li><code>[Case: You have Training Code]</code>: Corresponds to Scenario 2 below.</li> </ul> </li> <li>Configure: Fill in the required fields (e.g., Project Name, Absolute Path to Data).</li> <li>Launch: Click \ud83d\ude80 Run Setup Script.<ul> <li>The integrated console will show the setup progress as it creates directories, splits data, and generates the initial <code>evaluator.py</code>.</li> </ul> </li> </ol>"},{"location":"protocols/auto-setup/#scenario-1-data-only-ml_autosetup_1","title":"Scenario 1: Data-Only (<code>ml_autosetup_1</code>)","text":"<p>Use Case: You have a dataset (<code>X.npy</code>, <code>Y.npy</code>) but no model code. You want the Agent to build a model from scratch.</p>"},{"location":"protocols/auto-setup/#workflow","title":"Workflow","text":"<ol> <li>Input: Path to your data directory.</li> <li>Splitting: The script runs <code>split_data.py</code> to create <code>X_train.npy</code>, <code>X_test.npy</code>, etc.</li> <li>Generation: The Agent generates an initial <code>strategy.py</code> and <code>metric.py</code> based on your description.</li> <li>Verification: Runs a dry run to ensure the generated code runs.</li> </ol>"},{"location":"protocols/auto-setup/#scenario-2-bring-your-own-code-ml_autosetup_2","title":"Scenario 2: Bring Your Own Code (<code>ml_autosetup_2</code>)","text":"<p>Use Case: You already have a training script (<code>strategy.py</code>) and want the Agent to optimize hyperparameters or architecture.</p>"},{"location":"protocols/auto-setup/#requirements-the-byoc-protocol","title":"Requirements (The BYOC Protocol)","text":"<p>To use this mode, your code must adhere to a simple interface contract so the Evaluator can judge it:</p> <ol> <li>Weight Saving: Your script must save the best model weights to a file (e.g., <code>best_fast.pt</code>).</li> <li>Loading Interface: You must implement a factory function:     <pre><code>def load_trained_model(path, device):\n    # Return the loaded model object\n    return model\n</code></pre></li> <li>Data Protocol: Your code should load data using the shared <code>experiment_setup.py</code> module (generated by the wizard) to ensure train/test splits are consistent between the Player (Strategy) and the Judge (Evaluator).</li> </ol>"},{"location":"protocols/auto-setup/#what-it-generates","title":"What it Generates","text":"<ul> <li><code>experiment_setup.py</code>: Locks the random seed and data splits. Immutable.</li> <li><code>evaluator_ref.py</code>: A template evaluator that loads your model and tests it against the reserved test set.</li> </ul>"},{"location":"protocols/auto-setup/#common-features","title":"Common Features","text":"<ul> <li>Metric Standardization: Both modes verify that the evaluator prints <code>Best metric: {val}</code> (lowercase 'm') for the workflow to parse.</li> <li>Safety Checks: Both modes include anti-cheating checks (e.g., verifying <code>y_test</code> was not modified in memory).</li> </ul>"},{"location":"protocols/byoc-protocol/","title":"BYOC Protocol (Bring Your Own Code)","text":"<p>To integrate your existing training code with AgentCommander, follow this contract.</p>"},{"location":"protocols/byoc-protocol/#1-interface","title":"1. Interface","text":"<p>Your <code>strategy.py</code> must save weights and be loadable by the evaluator.</p> <pre><code># strategy.py\ndef load_trained_model(path, device):\n    model = MyModel()\n    model.load_state_dict(torch.load(path))\n    return model\n</code></pre>"},{"location":"protocols/byoc-protocol/#2-evaluation","title":"2. Evaluation","text":"<p>Your <code>evaluator.py</code> must print exactly: <code>Best metric: 0.123</code> (Lower case 'm').</p>"},{"location":"protocols/byoc-protocol/#3-data-safety","title":"3. Data Safety","text":"<p>Do not modify test data in memory. The system includes anti-cheating checks that will invalidate your score if you do.</p>"},{"location":"reference/cli-guide/","title":"CLI Reference &amp; Tips","text":"<p>AgentCommander relies on the underlying CLI tools for model inference. Optimizing their configuration can significantly improve performance and stability.</p>"},{"location":"reference/cli-guide/#gemini-cli","title":"Gemini CLI","text":"<ul> <li>Installation: <code>npm install -g @google/gemini-cli@latest</code></li> <li>Authentication: Run <code>gemini login</code> to authenticate with your Google account.</li> </ul>"},{"location":"reference/cli-guide/#enabling-preview-models","title":"\ud83d\ude80 Enabling Preview Models","text":"<p>The standard models (e.g., <code>gemini-pro</code>) are stable but may lag behind in reasoning capability. For complex coding tasks, we recommend Gemini 3 Pro or Flash 3.</p> <p>To access them: 1.  Ensure you have the latest CLI version. 2.  When configuring the Agent in the UI, type the model alias manually if it doesn't appear in the dropdown (e.g., <code>gemini-3-pro-latest</code>).</p>"},{"location":"reference/cli-guide/#context-isolation-important","title":"\ud83d\udee1\ufe0f Context Isolation (Important)","text":"<p>By default, the CLI maintains a history file (<code>~/.gemini/GEMINI.md</code>). If multiple projects write to this file, context pollution can occur.</p> <p>Tip: The AgentCommander UI automatically handles this isolation by setting the working directory as the context root, but for global CLI usage, consider setting your global <code>GEMINI.md</code> to Read-Only to prevent accidental contamination from manual CLI usage.</p>"},{"location":"reference/cli-guide/#qwen-cli","title":"Qwen CLI","text":"<ul> <li>Installation: <code>npm install -g @qwen/cli</code></li> </ul>"},{"location":"reference/cli-guide/#free-tier-oauth","title":"\ud83c\udd93 Free Tier (OAuth)","text":"<p>Qwen offers an \"OpenAI-compatible, OAuth free tier\" which provides ~2,000 free requests/day. 1.  Run <code>qwen login</code>. 2.  Follow the OAuth flow. 3.  This is excellent for long-running evolutionary tasks where token costs on other platforms might be prohibitive.</p>"},{"location":"reference/cli-guide/#opencode-ai","title":"OpenCode AI","text":"<p>An open-source oriented backend.</p> <ul> <li>Installation: <code>npm install -g opencode-ai</code></li> <li>Configuration: Run <code>opencode login</code> to authenticate.</li> </ul>"},{"location":"reference/cli-guide/#claude-code-anthropic","title":"Claude Code (Anthropic)","text":"<p>Integration with Anthropic's official CLI.</p> <ul> <li>Installation: <code>npm install -g @anthropic-ai/claude-code</code></li> <li>Configuration:<ol> <li>Run <code>claude login</code>.</li> <li>This will open a browser window to authenticate with your Anthropic Console account.</li> <li>Grant permission to the CLI.</li> </ol> </li> </ul>"},{"location":"reference/cli-guide/#common-issues","title":"Common Issues","text":""},{"location":"reference/cli-guide/#model-not-found","title":"\"Model not found\"","text":"<ul> <li>Cause: Your CLI version is outdated.</li> <li>Fix: Run <code>npm update -g @google/gemini-cli</code> (or <code>@qwen/cli</code>).</li> </ul>"},{"location":"reference/cli-guide/#nodejs-warnings","title":"Node.js Warnings","text":"<ul> <li>If you see warnings about experimental fetch APIs, upgrade Node.js to the latest LTS (Long Term Support) version.</li> </ul>"},{"location":"reference/configuration/","title":"Configuration (<code>config.json</code>)","text":"<p>The <code>config.json</code> file controls the global behavior.</p>"},{"location":"reference/configuration/#key-fields","title":"Key Fields","text":"<ul> <li>root_dir: Working directory.</li> <li>n_cycles: Number of iterations.</li> <li>global_vars: Variables accessible in prompt templates via <code>{{ var_name }}</code>.<ul> <li><code>eval_cmd</code>: The shell command to run evaluation.</li> <li><code>plot_names</code>: Filenames of plots to analyze.</li> </ul> </li> </ul>"},{"location":"reference/tips/","title":"Tips &amp; Best Practices","text":""},{"location":"reference/tips/#optimizing-ml-parameter-search","title":"Optimizing ML Parameter Search","text":"<p>For a good balance between speed and cost, a duration of 20-30 minutes for an experiment cycle (including parameter searching) is recommended. This allows sufficient exploration without excessive expenditure.</p>"},{"location":"reference/tips/#prompt-templating","title":"Prompt Templating","text":"<p>Use Jinja2-like syntax (e.g., <code>{{ variable_name }}</code>) in <code>llm_generate</code> nodes to dynamically inject values from the shared context.</p>"},{"location":"reference/tips/#debugging-with-cli","title":"Debugging with CLI","text":"<p>Use <code>gemini -r</code>, <code>qwen -c</code>, or <code>opencode -c</code> inside an experiment directory to inspect the exact prompt sent to the LLM and resume the conversation manually if needed.</p>"},{"location":"reference/tips/#cli-hygiene","title":"CLI Hygiene","text":"<p>Set your global CLI history file (e.g., <code>~/gemini/GEMINI.md</code>) to Read-Only to prevent context pollution between different projects.</p>"},{"location":"reference/troubleshooting/","title":"Troubleshooting","text":""},{"location":"reference/troubleshooting/#no-console-output-in-ui","title":"No Console Output in UI","text":"<p>If the \"Console Output\" box in the UI is empty but the agent seems to be running:</p> <ol> <li>Check Backend Terminal: Look at the terminal where you started <code>run_ui.sh</code>. Do you see logs there?<ul> <li>AgentCommander uses Dual Logging: Logs are printed to the terminal and sent to the UI.</li> </ul> </li> <li>Browser Refresh: If the terminal has logs but the UI doesn't, the WebSocket connection likely dropped. Refresh the page to reconnect.</li> <li>Check \"Console\" Visibility: Ensure you are on the Control tab. The console is located at the bottom of the Control Panel.</li> </ol>"},{"location":"reference/troubleshooting/#run-task-error-unexpected-keyword-argument-no_exec_list","title":"\"Run Task\" Error: Unexpected keyword argument 'no_exec_list'","text":"<p>This error indicates a mismatch between the running <code>ui_server.py</code> and the installed <code>pylib</code>.</p> <ul> <li>Fix: Restart the UI Server (<code>Ctrl+C</code> then <code>bash run_ui.sh</code>). Python code changes in the library require a restart to be loaded.</li> </ul>"},{"location":"reference/troubleshooting/#metric-not-found-infinite-score","title":"Metric Not Found / Infinite Score","text":"<p>If the agent keeps failing with \"Metric is INF\":</p> <ol> <li>Check Evaluator Output: Use the \"White-box Debugging\" method to inspect <code>eval_out.txt</code> in the experiment directory.</li> <li>Check Print Format: Ensure your <code>evaluator.py</code> prints exactly <code>Best metric: 0.123</code> (Case sensitive: lowercase 'm' in metric).</li> <li>Check Leakage: If the Anti-Cheating check fails, the score is forced to Infinity. Check if your model is modifying <code>y_test</code> in place.</li> </ol>"},{"location":"reference/troubleshooting/#windows-wsl2-issues","title":"Windows WSL2 Issues","text":"<ul> <li>Accessing UI: If <code>localhost:8080</code> doesn't work, try the WSL IP address (run <code>wsl hostname -I</code>).</li> <li>Permissions: Ensure your project folder is inside the WSL filesystem (e.g., <code>~/projects/</code>), not on the mounted Windows drive (<code>/mnt/c/</code>), for better performance and permission handling.</li> </ul>"}]}